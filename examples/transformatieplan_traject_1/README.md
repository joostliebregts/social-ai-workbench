# Transformation trajectory 1: From day-long processing to 1-minute synthesis

*What happens when AI can create concept documents directly from conversation*

## The recognition moment

Picture this: 20-30 stakeholders from municipalities and care organizations sitting around tables covered with sticky notes. The facilitator is collecting discussion points, making sure everyone's voice gets heard. Standard process.

Then comes the part everyone knows but rarely talks about: after the session, the project team disappears for a full day to process all those sticky notes and scattered thoughts into something resembling a coherent concept document. That document goes to a 10-person writing team for feedback. Changes get made. The cycle repeats.

I was listening to a project leader describe exactly this process when it hit me: "What if AI could create that concept document directly from the conversation?"

Her response: "I'd like to try that once."

## What we discovered

The facilitator put her phone on the table for Dembrane transcription—nothing disruptive, just there. During natural session breaks (bathroom, coffee), we processed the transcript in real-time. The AI output got copied to a Google Doc and shown on screen via beamer.

**1 minute instead of 1 day processing.**

The stakeholders saw their own language reflected back immediately. Not some sanitized version written by people who weren't in the room, but their actual words and phrases organized into something coherent. The recognizability was everything—these people needed to implement the plan, so they needed to see themselves in it.

**Example**: After 45 minutes of vision discussion about "Exploratory Conversations," the AI structured their scattered thoughts into a coherent vision statement: "By 2029, the Exploratory Dialogue has evolved into a low-threshold, integrated access to support..." The room went quiet. Then: "Yes, that captures exactly what we meant." Their own thinking reflected back, but organized.

## How it actually worked

Technically, this was simple:

- **Dembrane transcription**: Phone on table, real-time capture of multi-conversation streams
- **Strategic timing**: Processing during natural breaks, not interrupting flow
- **Parallel workflow**: AI working alongside traditional sticky note process
- **Visual integration**: Output displayed via beamer for group review
- **Immediate iteration**: Group feedback processed on the spot

The sticky notes stayed crucial. People still needed visualization and co-creation during the session. AI didn't replace the human process—it worked parallel to it.

## What this trajectory contains

This was our first experiment, so the prompts are simpler than later versions. You'll find:

### Session 1 (`sessie_1/`)
Three concrete stories from the first live experiment:

- **[Vision Feedback story](sessie_1/visie_terugkoppeling/README.md)** - From abstract conversation to tangible validation: 45 minutes of vision discussion became a structured document in 30 seconds
- **[Echo Question story](sessie_1/vraag_voor_volgende_groep/README.md)** - 10-second bridge building between rotating groups: AI captured one group's essence as a sharp starting question for the next
- **[Theme Synthesis story](sessie_1/synthese_per_thema/README.md)** - From sticky notes to coherent themes: End-of-day processing that turned scattered thoughts into implementation-ready concepts

### Session 2 (`sessie_2/`)  
- **Shared narrative development**: Building collective story from individual inputs
- **Feedback processing workflows**: Multiple rounds of group input integration
- **Discussion question generators**: Creating engagement for follow-up sessions
- **Golden standard templates**: Quality frameworks for group decisions

### Asynchronous processing (`asynchroon/`)
- Between-session synthesis approaches
- Participant preparation materials

## Try this yourself

Want to experiment with AI-facilitation collaboration? Start with [Session 1](sessie_1/README.md)—it shows exactly what happened in our first live test. One facilitator's phone, three prompts, and a beamer to show results to the group.

The stories above show you what worked (and why it worked) in actual high-stakes conversations with Dutch mental health organizations. You don't need AI experience to try these approaches—the prompts are designed for facilitators who've never used AI before.

**Key insight**: AI doesn't replace your facilitation skills—it amplifies them by catching patterns while you stay present with the group.

## The breakthrough insight

**Timing beats sophistication.** The AI output didn't need to be perfect—it needed to be timely and recognizable. When stakeholders saw their own language organized into coherent form within minutes of speaking it, something shifted. Instead of feeling replaced by technology, they felt heard by it.

The facilitator stayed fully present with the group—reading faces, holding emotions, guiding process. The AI caught patterns and synthesized content. Everyone did what they do best.

## What happened next

This first trajectory taught us that transparency creates trust. When participants could see exactly how their input became synthesis, they engaged differently. No black box, no magic—just their words, organized thoughtfully, available immediately.

The project leader got her concept document. The stakeholders saw themselves in it. The implementation had buy-in from day one.

---

**Context note**: This trajectory represents the Social AI Workbench v0.2 focus on accessibility—making AI collaboration approachable for people who've never used AI before. The prompts here are intentionally simpler than later versions, showing the evolution of human-AI collaboration in live facilitation.

*Part of the [Social AI Workbench](../../README.md) — transparent learning about human-AI collaboration in live meetings*