# CLAUDE.md - Prompt Ratings Directory

## Purpose

This subdirectory is planned to contain assessment frameworks and effectiveness metrics for evaluating prompts used in the Social AI Workbench. The goal is to develop practical ways of measuring what works in live collaborative AI contexts.

## Current Status

Currently contains a `SYSTEM.md` file outlining assessment approaches. In ship-day mode, the focus has been on collecting and using prompts rather than systematically rating them. Assessment frameworks will develop as more usage data accumulates.

## What Will Be Here

### Assessment Frameworks
- **Live facilitation metrics**: Speed, clarity, group engagement
- **Output quality measures**: Accuracy, transparency, actionability  
- **Context adaptation ratings**: How well prompts work across different situations
- **Facilitator usability scores**: Ease of integration into real sessions

### Effectiveness Data
- **Performance tracking**: Which prompts work best in which contexts
- **Comparative analysis**: How different approaches stack up against each other
- **Usage patterns**: Which prompts get used repeatedly vs. abandoned
- **Failure analysis**: Common failure modes and their frequency

### Evaluation Methods
- **Real-session feedback**: Direct input from facilitators and participants
- **Output analysis**: Quality assessment of generated content
- **Speed metrics**: Performance under real-time constraints  
- **Iteration tracking**: How prompts improve (or don't) over time

## Assessment Philosophy

### Practical Over Perfect
- Focus on what matters in live facilitation contexts
- Prioritize facilitator and participant feedback over abstract metrics
- Value real-world usability over theoretical elegance
- Measure impact on actual collaborative processes

### Context-Sensitive Evaluation  
- Recognize that prompt effectiveness depends heavily on context
- Track performance across different group sizes, cultures, and topics
- Document when and why prompts work or fail
- Avoid universal ratings that ignore situational factors

## Planned Rating Dimensions

### Technical Performance
- **Speed**: Response time under live session constraints
- **Accuracy**: Faithfulness to transcript content
- **Robustness**: Handling of incomplete or noisy input
- **Integration**: Compatibility with Dembrane workflow

### Facilitation Value
- **Clarity**: How well outputs communicate to groups
- **Actionability**: Whether outputs lead to productive next steps
- **Engagement**: Impact on group participation and energy
- **Transparency**: Clear sourcing and limitation communication

### Adaptability  
- **Context flexibility**: Performance across different situations
- **Language handling**: Effectiveness with Dutch/English content
- **Group size scaling**: Performance with small vs. large groups
- **Cultural sensitivity**: Appropriateness across different organizational contexts

## Data Sources

### Direct Feedback
- **Facilitator reports**: Post-session assessments from practitioners
- **Participant surveys**: Group feedback on AI-generated content
- **Session observations**: Notes from live facilitation experiences
- **Iteration documentation**: Changes made based on real usage

### Performance Metrics
- **Usage frequency**: Which prompts get selected repeatedly
- **Modification patterns**: How prompts get adapted in practice  
- **Success indicators**: Measurable outcomes from sessions using prompts
- **Failure documentation**: When and why prompts don't work

## Assessment Methodology

### Evidence-Based Rating
- All ratings must be grounded in actual usage data
- Multiple sessions required before establishing patterns
- Clear documentation of assessment context and constraints
- Regular re-evaluation as more evidence accumulates

### Stakeholder Perspectives
- **Facilitators**: Ease of use, integration with workflow
- **Participants**: Quality and usefulness of generated content
- **Project leads**: Alignment with overall objectives
- **Technical users**: Reliability and predictable performance

## Connection to Project Structure

### Input Sources
- **Raw prompts** from `/examples/` provide assessment subjects
- **Evolution stories** from `/iterations/` show improvement trajectories
- **Pattern insights** from `/lessons-learned/` inform rating criteria
- **Private transcripts** from `/context/conversations/` provide real usage context

### Output Applications
- **Prompt selection**: Help facilitators choose appropriate tools
- **Quality assurance**: Identify prompts needing improvement
- **Training materials**: Show examples of high and low-performing prompts
- **Community guidance**: Share insights about effective prompt characteristics

## For AI Assistants

When this directory becomes active:

### Creating Assessments
1. **Gather real usage data**: Base ratings on actual session experience
2. **Use multiple perspectives**: Include facilitator and participant viewpoints
3. **Document context**: Specify when and where assessments apply
4. **Show evidence**: Provide concrete examples supporting ratings

### Interpreting Ratings
- **Consider context**: Ratings may not transfer across different situations
- **Look for patterns**: Individual ratings matter less than trends
- **Account for evolution**: Prompts may improve over time
- **Balance perspectives**: Weight different stakeholder views appropriately

## Quality Standards

### Honest Assessment
- Include failures and limitations alongside successes
- Document edge cases and boundary conditions
- Acknowledge when data is insufficient for confident ratings
- Update ratings as new evidence becomes available

### Practical Focus
- Prioritize metrics that matter to real users
- Avoid over-engineering assessment systems
- Keep rating processes lightweight and sustainable
- Focus on actionable insights over perfect measurement

## Anti-Patterns to Avoid

- **Theoretical ratings**: Not grounded in real usage experience
- **Over-complex systems**: Assessment frameworks that nobody will actually use
- **False precision**: Claiming accuracy beyond what data supports
- **Context blindness**: Universal ratings that ignore situational factors
- **Academic focus**: Optimizing for research rather than practitioner value

## Long-Term Vision

This directory should eventually become:
- **A practical guide** for selecting appropriate prompts for specific contexts
- **A quality assurance system** for maintaining high prompt effectiveness
- **A research resource** contributing to understanding of collaborative AI assessment
- **A feedback mechanism** driving continuous improvement of prompt design

The goal is useful evaluation frameworks that help practitioners make better decisions about when and how to use different prompts in real collaborative contexts.